---
title: "Machine Learning Subreddit"
date: "2025-01-08"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "deep learning"]
---

# Overall Ranking and Top Discussions
1.  [[D] ML Engineers, what's the most annoying part of your job?](https://www.reddit.com/r/MachineLearning/comments/1hwbhuj/d_ml_engineers_whats_the_most_annoying_part_of/) (Score: 76)
    * This thread discusses the frustrating aspects of being an ML engineer, from data quality issues to misaligned expectations from management and other teams.
2.  [[R][D] White Box Transformers](https://www.reddit.com/r/MachineLearning/comments/1hvy385/rd_white_box_transformers/) (Score: 56)
    * This thread discusses a theory paper that tries to interpret ML/DL in specific frameworks, specifically around a new model architecture called CRATE ViT, which is similar to transformers and discusses SLERP.
3.  [[D] What is the most fascinating aspect of machine learning for you?](https://www.reddit.com/r/MachineLearning/comments/1hvqdvt/d_what_is_the_most_fascinating_aspect_of_machine/) (Score: 45)
    *   This thread explores what people find most interesting about machine learning, including the math, optimization, and emergent behaviors of models.
4.  [[D] Monthly Who's Hiring and Who wants to be Hired?](https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/) (Score: 36)
    * This is a monthly thread where people share job postings, or their resumes/profiles while looking for jobs in the ML/AI/data space.
5.  [[D] To Fellow researchers: What are your top 3 challenges in research?](https://www.reddit.com/r/MachineLearning/comments/1hwh8um/d_to_fellow_researchers_what_are_your_top_3/) (Score: 31)
    *   This thread discusses the top challenges researchers face, including defining good research questions, data limitations, computational resources, and reproducibility issues.
6.  [[D] ML engineers, what is the most rewarding thing about your job?](https://www.reddit.com/r/MachineLearning/comments/1hvx2pq/d_ml_engineers_what_is_the_most_rewarding_thing/) (Score: 22)
    * This thread explores what ML engineers find most rewarding about their jobs including financial impact, work-life balance, and the ability to solve interesting problems.
7.  [[D] Optimization techniques in NLP/LLM that also works in transformers based sequence modeling?](https://www.reddit.com/r/MachineLearning/comments/1hvj7fx/d_optimization_techniques_in_nlpllm_that_also/) (Score: 22)
    * This thread discusses different optimization techniques that can be used for both NLP/LLMs and transformers-based sequence modeling.
8.  [[R][P] distillKitPlus: High Performent Knowledge Distillation for LLMs](https://www.reddit.com/r/MachineLearning/comments/1hw0mn9/rp_distillkitplus_high_performent_knowledge/) (Score: 14)
    * This thread discusses the `distillKitPlus` tool for knowledge distillation for LLMs and its performance.
9.  [[D] Positional Embeddings in Embedding Space](https://www.reddit.com/r/MachineLearning/comments/1hvxpcc/d_positional_embeddings_in_embedding_space/) (Score: 13)
     * This thread discusses the different positional encodings (fixed vs RPE) and their distributions in embedding space.
10. [[D] Self-Promotion Thread](https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/) (Score: 8)
    * This is a self-promotion thread where people promote their work, projects, or products, including a synthetic data API, a curated listing platform, and research papers.
11. [[R][N] TabPFN v2: Accurate predictions on small data with a tabular foundation model](https://www.reddit.com/r/MachineLearning/comments/1hwvk9x/rn_tabpfn_v2_accurate_predictions_on_small_data/) (Score: 7)
    * This thread introduces TabPFN v2, a tabular foundation model that has accurate predictions on small data.
12.  [[R] LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks](https://www.reddit.com/r/MachineLearning/comments/1hwfs48/r_longbench_v2_towards_deeper_understanding_and/) (Score: 5)
     * This thread discusses the LongBench v2 benchmark for evaluating long-context models.
13. [[D] Anyone tried predibase/lorax?](https://www.reddit.com/r/MachineLearning/comments/1hwdw40/d_anyone_tried_predibaselorax/) (Score: 4)
    * This thread asks for feedback on experiences with Predibase/Lorax, a tool for serving multiple LORAs.
14. [[D][R] What conferences are on your list this year?](https://www.reddit.com/r/MachineLearning/comments/1hwpbnh/dr_what_conferences_are_on_your_list_this_year/) (Score: 2)
     * This thread discusses which ML conferences people are planning to attend this year and shares a list of ML conferences.
15.  [How do the real time TTS models work? [Discussion]](https://www.reddit.com/r/MachineLearning/comments/1hwmmze/how_do_the_real_time_tts_models_work_discussion/) (Score: 0)
    * This thread is about real-time Text-to-Speech (TTS) models and how they work, with mentions of specific models like Eleven Labs.
16.  [[D] How is developing internal LLMs going?](https://www.reddit.com/r/MachineLearning/comments/1hwxgqj/d_how_is_developing_internal_llms_going/) (Score: 0)
    * This thread discusses the progress of developing internal Large Language Models (LLMs), including various aspects of training and usage.


# Detailed Analysis by Thread
**[ [D] ML Engineers, what's the most annoying part of your job? (Score: 76)](https://www.reddit.com/r/MachineLearning/comments/1hwbhuj/d_ml_engineers_whats_the_most_annoying_part_of/)**
*  **Summary:** This thread discusses various pain points of ML engineers including data quality issues, conflicting expectations from management, non-ML teams, and the challenges of fitting ML workflows into standard software engineering processes.
*  **Emotion:** The emotional tone of the thread is largely **negative**, with comments expressing frustration, annoyance, and dissatisfaction. There are some instances of neutral sentiment but the dominant emotions are negative.
*  **Top 3 Points of View:**
    *  The most common annoyance is dealing with bad data. This includes issues such as missing data, incorrect data types, inconsistent data formats, and lack of proper inspection of the data before passing it down to ML Engineers.
    *  ML Engineers are frustrated with the lack of understanding from product owners, project managers, and other stakeholders who are not familiar with the ML process, including things like the time needed for exploring new ideas and the need to iterate.
    * ML engineers struggle to fit the iterative and experimental nature of ML projects into traditional software engineering workflows.

**[ [R][D] White Box Transformers (Score: 56)](https://www.reddit.com/r/MachineLearning/comments/1hvy385/rd_white_box_transformers/)**
*  **Summary:** This thread centers around a discussion of the "White Box Transformers" paper which presents a new framework for interpreting ML/DL models and their relationship to transformers, including the CRATE ViT framework. It touches on the mathematical theory behind it and potential applications.
*  **Emotion:** The emotional tone of the thread is generally **neutral to positive**, with users expressing interest in the research and finding it mathematically interesting. Some users note that the work might not lead to immediate practical applications.
*  **Top 3 Points of View:**
    *  Some view the paper as an interesting theoretical contribution to the understanding of ML/DL, particularly in how it relates to transformers and how it ties together concepts from DINO.
    *  Others express skepticism about the practical implications of this work, noting that similar theory papers often lack immediate real-world applications.
    *  There is a discussion on the mathematical aspects of the paper, specifically regarding SLERP and octonions.

**[ [D] What is the most fascinating aspect of machine learning for you? (Score: 45)](https://www.reddit.com/r/MachineLearning/comments/1hvqdvt/d_what_is_the_most_fascinating_aspect_of_machine/)**
*  **Summary:** This thread asks users what they find fascinating about machine learning. Many comments highlight the surprising effectiveness of simple algorithms, the emergent properties of complex models, and the mathematical underpinnings of the field.
*  **Emotion:** The emotional tone of this thread is **neutral to positive**. Users express fascination, amazement, and excitement about various aspects of machine learning.
*  **Top 3 Points of View:**
    *  Many users are fascinated by how complex behaviors can emerge from simple units in models.
    *  Some users are intrigued by the effectiveness of optimization techniques and the fact that programs can be created through optimization rather than through explicit construction.
    *  The ability of neural networks to learn complex functions is also seen as fascinating.

**[ [D] Monthly Who's Hiring and Who wants to be Hired? (Score: 36)](https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/)**
*  **Summary:** This is a recurring thread where individuals post job opportunities and express their interest in employment in the machine learning field. It serves as a platform for connecting employers and job seekers.
*  **Emotion:** The emotional tone of this thread is largely **neutral**. The posts are informative and transactional, and do not express strong emotions.
*  **Top 3 Points of View:**
    *  Job seekers post brief descriptions of their skills and experience, along with salary expectations.
    *  Employers post about open positions within their companies or research labs, with information about required skills and location.
    *  Both job seekers and employers provide links to their resumes or applications

**[ [D] To Fellow researchers: What are your top 3 challenges in research? (Score: 31)](https://www.reddit.com/r/MachineLearning/comments/1hwh8um/d_to_fellow_researchers_what_are_your_top_3/)**
*  **Summary:** In this thread, researchers discuss the main challenges they face in their work. This includes problems relating to scoping problems, finding good datasets, compute resources, and language barriers.
*  **Emotion:** The overall tone of the thread is **neutral** with some negative leaning. Researchers express frustration with challenges but without overly emotional language.
*  **Top 3 Points of View:**
    *  A major challenge is identifying and defining good research problems, including the scope. Researchers often find it difficult to come up with focused problem statements.
    *  A lack of good quality data is a significant challenge with data being hard to find or requiring manual collection via scraping.
    * Researchers highlight a lack of computational resources, and having to rely on flawed metrics to evaluate their work.


**[ [D] ML engineers, what is the most rewarding thing about your job? (Score: 22)](https://www.reddit.com/r/MachineLearning/comments/1hvx2pq/d_ml_engineers_what_is_the_most_rewarding_thing/)**
*  **Summary:** This thread discusses the most rewarding aspects of being an ML engineer. Comments highlight making a direct impact on company performance, solving challenging problems, and having a good balance between research and engineering.
*  **Emotion:** The overall tone is **positive**, with some neutral elements. The comments express satisfaction, enjoyment, and a sense of accomplishment in their work.
*  **Top 3 Points of View:**
    *  ML engineers find it rewarding when their models have a direct impact on the company's performance.
    *  Many enjoy the balance between software engineering, research and engineering.
    *  Getting the model to work as expected and not having to deal with pesky errors is another common source of satisfaction.

**[ [D] Optimization techniques in NLP/LLM that also works in transformers based sequence modeling? (Score: 22)](https://www.reddit.com/r/MachineLearning/comments/1hvj7fx/d_optimization_techniques_in_nlpllm_that_also/)**
*  **Summary:** This thread asks about optimization techniques used in NLP and LLMs that can also be used for general transformer-based sequence modeling. The discussion centers around data processing, specifically the concept of tokens and embeddings in different models and domains.
*  **Emotion:** The overall tone of this thread is **neutral**, with users asking for and sharing information.
*  **Top 3 Points of View:**
    *   The key difference between tokens in NLP/LLMs versus tokens in sequence modeling for other systems, like user sequences.
    *  The goal of tokenization in ML is to achieve a consistent information density across tokens.
    * The challenges of working with sparse embedding vectors and how that can be mitigated in the first layer using mean embedding or convolution.

**[ [R][P] distillKitPlus: High Performent Knowledge Distillation for LLMs (Score: 14)](https://www.reddit.com/r/MachineLearning/comments/1hw0mn9/rp_distillkitplus_high_performent_knowledge/)**
*  **Summary:** This thread discusses a tool called `distillKitPlus` designed for knowledge distillation of LLMs, including how to use the tool and what kind of performance can be achieved.
*  **Emotion:** The emotional tone of this thread is generally **positive to neutral**. The creator of the tool is promoting it, and others are expressing interest.
*  **Top 3 Points of View:**
    *  The tool enables computing logits for 200 samples from llama models with a 4k sequence length for under $30 on modal labs.
     *  The creator is encouraging others to try the tool with an offer of support for anyone who has issues.
    *  Others are requesting benchmarks to support the performance claims.

**[ [D] Positional Embeddings in Embedding Space (Score: 13)](https://www.reddit.com/r/MachineLearning/comments/1hvxpcc/d_positional_embeddings_in_embedding_space/)**
*  **Summary:** This discussion revolves around positional encodings, specifically fixed sinusoidal embeddings and Relative Positional Encodings (RPE). The discussion includes their mathematical properties and the effects of normalization.
*  **Emotion:** The overall tone of this thread is **neutral**, with users discussing the technical details.
*  **Top 3 Points of View:**
    *  Fixed sinusoidal embeddings are distributed as hyper-rings.
    *  RPE treats QK vectors as complex numbers, rotating them in the imaginary plane and leaving their magnitude invariant.
    *  Normalization plays a key role in numerical stability and controlling the scale of embeddings.

**[ [D] Self-Promotion Thread (Score: 8)](https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/)**
*  **Summary:** This is a self-promotion thread where various users are promoting their projects, tools, services, and research papers related to machine learning.
*  **Emotion:** The overall tone of this thread is **positive to neutral**, with people sharing their work and research.
*  **Top 3 Points of View:**
    * A data-efficient synthetic data generation API that outperforms other methods, including with a 1.1B parameter model.
    * A curated listing platform for AI agents and platforms.
    *  Several research papers are promoted, including work on token reduction, reward learning for RLHF, and job industry insights.

**[ [R][N] TabPFN v2: Accurate predictions on small data with a tabular foundation model (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1hwvk9x/rn_tabpfn_v2_accurate_predictions_on_small_data/)**
*   **Summary:** This thread is about TabPFN v2, a tabular foundation model designed for accurate predictions on small datasets.
*   **Emotion:** The emotional tone of the thread is **positive**.
*   **Top 3 Points of View:**
    * One user expresses excitement about TabPFN v2 and plans to test it on their company data.

**[ [R] LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1hwfs48/r_longbench_v2_towards_deeper_understanding_and/)**
*  **Summary:** This thread discusses LongBench v2, a benchmark for evaluating long-context models, and provides user opinions on the results.
*  **Emotion:** The emotional tone is mixed with **both positive and negative** feedback.
*  **Top 3 Points of View:**
    * One user notes disappointment that Gemini was not considered in the benchmarks.
    *  Another user expresses surprise at the low rating of Sonnet in the benchmarks, which may affect their future decision making about choosing between models.

**[ [D] Anyone tried predibase/lorax? (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1hwdw40/d_anyone_tried_predibaselorax/)**
*  **Summary:** This thread asks for feedback on user experiences with `predibase/lorax`.
*  **Emotion:** The emotional tone is generally **negative**, based on user feedback.
*  **Top 3 Points of View:**
    * Some users experienced poor performance with Predibase/Lorax with a user facing RAG use case.
    *  Users found that vLLM performed better in the same scenario.
    *  The general consensus seems to be disappointment with the performance.

**[ [D][R] What conferences are on your list this year? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1hwpbnh/dr_what_conferences_are_on_your_list_this_year/)**
*  **Summary:** This thread asks about which conferences people are planning on attending this year.
*  **Emotion:** The emotional tone of the thread is **neutral to positive**.
*  **Top 3 Points of View:**
    * One user is asking about a tracker of major conferences across all fields.
    * Another user shared a link to aideadlin.es with a list of ML conferences.
    * One user says thanks.

**[How do the real time TTS models work? [Discussion] (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1hwmmze/how_do_the_real_time_tts_models_work_discussion/)**
*  **Summary:** This thread is a discussion about how real-time Text-to-Speech models work.
*  **Emotion:** The emotional tone of this thread is **neutral** as it's mostly a technical discussion.
* **Top 3 Points of View:**
    * One user specifically mentions "eleven labs" as a relevant model.
    * Another user suggests that it uses a latent diffusion model in time space.
     *  One user asks about the types of models being used for TTS.

**[ [D] How is developing internal LLMs going? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1hwxgqj/d_how_is_developing_internal_llms_going/)**
* **Summary:** This thread inquires about the progress of developing internal LLMs and mentions that r/LLMDevs may be a better subreddit for this discussion.
* **Emotion:** The emotional tone of this thread is **neutral**.
* **Top 3 Points of View:**
    * A user recommends that the question is asked on r/LLMDevs.
     * The user asks if the intent of the question was very broad, and provides examples of different ways an LLM can be trained, and suggests that RAG is a common technique used when discussing internal LLMs.
